import torch
import os
import tiktoken
import time
import math
from src.model import YuiGPT, BATCH_SIZE, BLOCK_SIZE

# --- –ì–Ü–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–ò ---
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
LEARNING_RATE = 3e-4
MAX_ITERS = 3000        # –ë—ñ–ª—å—à–µ —ñ—Ç–µ—Ä–∞—Ü—ñ–π, –±–æ —î scheduler
EVAL_INTERVAL = 100     # –Ø–∫ —á–∞—Å—Ç–æ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ on validation set
EVAL_ITERS = 50         # –°–∫—ñ–ª—å–∫–∏ –∫—Ä–æ–∫—ñ–≤ —É—Å–µ—Ä–µ–¥–Ω—é–≤–∞—Ç–∏ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏

def main():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –ü–†–û–§–ï–°–Ü–ô–ù–û–ì–û –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {DEVICE}...")
    
    # 1. –ß–∏—Ç–∞—î–º–æ –¥–∞–Ω—ñ
    if not os.path.exists('data/input.txt'):
        print("‚ùå –ù–µ–º–∞—î data/input.txt! –ó–∞–ø—É—Å—Ç–∏ —Å–ø–æ—á–∞—Ç–∫—É setup_data.py")
        return

    with open('data/input.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    # 2. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
    print("üß† –ö–æ–¥—É—î–º–æ —Ç–µ–∫—Å—Ç (Tiktoken BPE)...")
    enc = tiktoken.get_encoding("cl100k_base")
    vocab_size = enc.n_vocab
    
    data_ids = enc.encode_ordinary(text)
    data = torch.tensor(data_ids, dtype=torch.long)
    n = int(0.9 * len(data)) # 90% –Ω–∞–≤—á–∞–Ω–Ω—è, 10% —Ç–µ—Å—Ç
    train_data = data[:n]
    val_data = data[n:]
    
    print(f"üìö –í—Å—å–æ–≥–æ —Ç–æ–∫–µ–Ω—ñ–≤: {len(data)}")
    print(f"üéì Train set: {len(train_data)} | üß™ Val set: {len(val_data)}")

    # 3. –ë–∞—Ç—á—ñ–Ω–≥
    def get_batch(split):
        data_source = train_data if split == 'train' else val_data
        ix = torch.randint(len(data_source) - BLOCK_SIZE, (BATCH_SIZE,))
        x = torch.stack([data_source[i:i+BLOCK_SIZE] for i in ix])
        y = torch.stack([data_source[i+1:i+BLOCK_SIZE+1] for i in ix])
        return x.to(DEVICE), y.to(DEVICE)

    # 4. –§—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ (–±–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è, —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞)
    @torch.no_grad()
    def estimate_loss(model):
        out = {}
        model.eval()
        for split in ['train', 'val']:
            losses = torch.zeros(EVAL_ITERS)
            for k in range(EVAL_ITERS):
                X, Y = get_batch(split)
                logits, loss = model(X, Y)
                losses[k] = loss.item()
            out[split] = losses.mean()
        model.train()
        return out

    # 5. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    model = YuiGPT(vocab_size).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # Scheduler: –ø–ª–∞–≤–Ω–æ –∑–º–µ–Ω—à—É—î LR –¥–æ 10% –≤—ñ–¥ –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_ITERS, eta_min=LEARNING_RATE/10)
    
    # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç, —è–∫—â–æ —î
    best_val_loss = float('inf')
    if os.path.exists('models/yui_best.pth'):
        print("üì• –ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –Ω–∞–π–∫—Ä–∞—â–∏–π —á–µ–∫–ø–æ—ñ–Ω—Ç. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é...")
        try:
            model.load_state_dict(torch.load('models/yui_best.pth', map_location=DEVICE))
            # –û—Ü—ñ–Ω–∏–º–æ –π–æ–≥–æ
            losses = estimate_loss(model)
            best_val_loss = losses['val']
            print(f"   –ü–æ—Ç–æ—á–Ω–∏–π best_val_loss: {best_val_loss:.4f}")
        except:
            print("   ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è, –≤—á–∏–º–æ –∑ –Ω—É–ª—è.")

    # 6. –¶–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è
    print("\nüèÅ –ü–æ—ó—Ö–∞–ª–∏!")
    start_time = time.time()
    
    for iter in range(MAX_ITERS):
        # –û—Ü—ñ–Ω–∫–∞
        if iter % EVAL_INTERVAL == 0:
            losses = estimate_loss(model)
            dt = time.time() - start_time
            print(f"Step {iter}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f} [Time: {dt:.1f}s]")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
            if losses['val'] < best_val_loss:
                best_val_loss = losses['val']
                if not os.path.exists('models'): os.makedirs('models')
                torch.save(model.state_dict(), 'models/yui_best.pth')
                print(f"   üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–æ–≤—É –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å! (Loss: {best_val_loss:.4f})")

        # –ù–∞–≤—á–∞–ª—å–Ω–∏–π –∫—Ä–æ–∫
        xb, yb = get_batch('train')
        logits, loss = model(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
        scheduler.step()

    # –§—ñ–Ω–∞–ª—å–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    torch.save(model.state_dict(), 'models/yui_final.pth')
    print("\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–ù–∞–π–∫—Ä–∞—â–∏–π Val Loss: {best_val_loss:.4f}")

if __name__ == "__main__":
    main()
